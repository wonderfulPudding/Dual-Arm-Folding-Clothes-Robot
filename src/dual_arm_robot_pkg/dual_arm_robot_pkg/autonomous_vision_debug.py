#!/usr/bin/env python3
"""
ì´ë¯¸ì§€ ê¸°ë°˜ ìë™ì œì–´ (ë””ë²„ê·¸ ë²„ì „)
"""

import torch
import torch.nn as nn
import numpy as np
import cv2
import time
import glob
from pathlib import Path
from dynamixel_sdk import *

print("=== Vision-based Autonomous Control (DEBUG) ===\n")

CHECKPOINT_PATH = Path("~/handkerchief_checkpoints/vision_best_model.pth").expanduser()
PROTOCOL_VERSION = 1.0
BAUDRATE = 1000000

FOLLOWER1_IDS = list(range(1, 7))
FOLLOWER2_IDS = list(range(11, 17))

ADDR_TORQUE_ENABLE = 24
ADDR_GOAL_POSITION = 30
ADDR_PRESENT_POSITION = 36
ADDR_MOVING_SPEED = 32

MOTOR_SPEED = 600
CAMERA_INDEX = 4
IMAGE_SIZE = 224


def detect_ports():
    usb_ports = sorted(glob.glob("/dev/ttyUSB*"))
    config = {}
    if len(usb_ports) >= 2:
        config['follower1'] = usb_ports[0]
        config['follower2'] = usb_ports[1]
    return config


class VisionPolicy(nn.Module):
    def __init__(self):
        super().__init__()
        
        self.cnn = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=2),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=2),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((7, 7)),
            nn.Flatten(),
        )
        
        self.img_feat_size = 128 * 7 * 7
        
        self.state_encoder = nn.Sequential(
            nn.Linear(24, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 128),
            nn.ReLU(),
        )
        
        self.decoder = nn.Sequential(
            nn.Linear(self.img_feat_size + 128, 512),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 12),
        )
    
    def forward(self, image, state):
        img_feat = self.cnn(image)
        state_feat = self.state_encoder(state)
        combined = torch.cat([img_feat, state_feat], dim=1)
        action = self.decoder(combined)
        return action


class VisionController:
    def __init__(self):
        self.ports = detect_ports()
        print(f"í¬íŠ¸ ì„¤ì •:")
        print(f"  íŒ”ë¡œì›Œ1: {self.ports['follower1']}")
        print(f"  íŒ”ë¡œì›Œ2: {self.ports['follower2']}\n")
        
        self.ph_follower1 = None
        self.ph_follower2 = None
        self.pkt_handler = None
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ë””ë°”ì´ìŠ¤: {self.device}\n")
        
        self.model = VisionPolicy().to(self.device)
        
        if CHECKPOINT_PATH.exists():
            checkpoint = torch.load(CHECKPOINT_PATH, map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            print(f"âœ“ ëª¨ë¸ ë¡œë“œ: epoch {checkpoint.get('epoch', 'N/A')}, loss {checkpoint.get('loss', 'N/A'):.4f}\n")
            self.model.eval()
        else:
            print(f"âœ— ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ: {CHECKPOINT_PATH}\n")
            exit(1)
        
        self.prev_qpos = None
        
        self.camera = cv2.VideoCapture(CAMERA_INDEX)
        if not self.camera.isOpened():
            print(f"âœ— ì¹´ë©”ë¼ ì—´ê¸° ì‹¤íŒ¨!")
            exit(1)
        
        self.camera.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)
        self.camera.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)
        print(f"âœ“ ì¹´ë©”ë¼: /dev/video{CAMERA_INDEX}\n")
        
        cv2.namedWindow('Vision Control', cv2.WINDOW_NORMAL)
        cv2.resizeWindow('Vision Control', 960, 540)
    
    def connect_robot(self):
        print("[ë¡œë´‡ ì—°ê²°]")
        self.pkt_handler = PacketHandler(PROTOCOL_VERSION)
        
        self.ph_follower1 = PortHandler(self.ports['follower1'])
        if not self.ph_follower1.openPort():
            raise Exception(f"íŒ”ë¡œì›Œ1 í¬íŠ¸ ì—´ê¸° ì‹¤íŒ¨!")
        self.ph_follower1.setBaudRate(BAUDRATE)
        print(f"  âœ“ íŒ”ë¡œì›Œ1 ì—°ê²°")
        
        self.ph_follower2 = PortHandler(self.ports['follower2'])
        if not self.ph_follower2.openPort():
            raise Exception(f"íŒ”ë¡œì›Œ2 í¬íŠ¸ ì—´ê¸° ì‹¤íŒ¨!")
        self.ph_follower2.setBaudRate(BAUDRATE)
        print(f"  âœ“ íŒ”ë¡œì›Œ2 ì—°ê²°\n")
        
        print("[ëª¨í„° í† í¬ í™œì„±í™”]")
        
        for mid in FOLLOWER1_IDS:
            self.pkt_handler.write1ByteTxRx(self.ph_follower1, mid, ADDR_TORQUE_ENABLE, 1)
            self.pkt_handler.write2ByteTxRx(self.ph_follower1, mid, ADDR_MOVING_SPEED, MOTOR_SPEED)
        
        for mid in FOLLOWER2_IDS:
            self.pkt_handler.write1ByteTxRx(self.ph_follower2, mid, ADDR_TORQUE_ENABLE, 1)
            self.pkt_handler.write2ByteTxRx(self.ph_follower2, mid, ADDR_MOVING_SPEED, MOTOR_SPEED)
        
        print("  âœ“ í† í¬ í™œì„±í™”\n")
    
    def disconnect_robot(self):
        if self.ph_follower1:
            self.ph_follower1.closePort()
        if self.ph_follower2:
            self.ph_follower2.closePort()
        if self.camera:
            self.camera.release()
        cv2.destroyAllWindows()
    
    def get_current_state(self):
        f1_pos = []
        for mid in FOLLOWER1_IDS:
            pos, _, _ = self.pkt_handler.read2ByteTxRx(self.ph_follower1, mid, ADDR_PRESENT_POSITION)
            f1_pos.append(pos if pos else 512)
        
        f2_pos = []
        for mid in FOLLOWER2_IDS:
            pos, _, _ = self.pkt_handler.read2ByteTxRx(self.ph_follower2, mid, ADDR_PRESENT_POSITION)
            f2_pos.append(pos if pos else 512)
        
        qpos = np.array(f1_pos + f2_pos, dtype=np.float32)
        
        if self.prev_qpos is None:
            qvel = np.zeros(12, dtype=np.float32)
        else:
            qvel = qpos - self.prev_qpos
        
        self.prev_qpos = qpos.copy()
        state = np.concatenate([qpos, qvel])
        
        return state, qpos
    
    def preprocess_image(self, frame):
        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))
        img = img.astype(np.float32) / 255.0
        img = np.transpose(img, (2, 0, 1))
        return torch.from_numpy(img).unsqueeze(0).to(self.device)
    
    def execute_action(self, action):
        action_np = action.cpu().numpy()
        
        for i, mid in enumerate(FOLLOWER1_IDS):
            pos = int(np.clip(action_np[i], 0, 1023))
            self.pkt_handler.write2ByteTxRx(self.ph_follower1, mid, ADDR_GOAL_POSITION, pos)
        
        for i, mid in enumerate(FOLLOWER2_IDS):
            pos = int(np.clip(action_np[i+6], 0, 1023))
            self.pkt_handler.write2ByteTxRx(self.ph_follower2, mid, ADDR_GOAL_POSITION, pos)
        
        return action_np
    
    def run(self, duration=30.0):
        print("[ì´ë¯¸ì§€ ê¸°ë°˜ ìë™ì œì–´ ì‹œì‘ - ë””ë²„ê·¸ ëª¨ë“œ]")
        print(f"  ì‹œê°„: {duration}ì´ˆ\n")
        
        start_time = time.time()
        step = 0
        
        try:
            while (time.time() - start_time) < duration:
                ret, frame = self.camera.read()
                if not ret:
                    print("  âœ— í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨!")
                    break
                
                state, current_qpos = self.get_current_state()
                
                img_tensor = self.preprocess_image(frame)
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                
                with torch.no_grad():
                    action_tensor = self.model(img_tensor, state_tensor).squeeze(0)
                
                action_np = self.execute_action(action_tensor)
                
                # ğŸ” ë””ë²„ê·¸ ì¶œë ¥
                if step % 10 == 0:
                    print(f"\n[ìŠ¤í… {step}]")
                    print(f"  í˜„ì¬ ìœ„ì¹˜ (qpos): {current_qpos[:6].astype(int)}")
                    print(f"  ì˜ˆì¸¡ í–‰ë™ (action): {action_np[:6].astype(int)}")
                    print(f"  ì°¨ì´ (delta): {(action_np[:6] - current_qpos[:6]).astype(int)}")
                
                display_frame = frame.copy()
                cv2.putText(display_frame, f"Step: {step}", (20, 50),
                          cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                cv2.putText(display_frame, f"Action: {action_np[0]:.0f}", (20, 100),
                          cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
                
                cv2.imshow('Vision Control', display_frame)
                
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q') or key == 27:
                    print("\nâœ— ì¤‘ë‹¨")
                    break
                
                step += 1
                time.sleep(0.03)
        
        except KeyboardInterrupt:
            print("\nâœ— Ctrl+C")
        
        print(f"\nâœ“ ì™„ë£Œ ({step}ìŠ¤í…)\n")


def main():
    print("="*60)
    print("ì´ë¯¸ì§€ ê¸°ë°˜ ìë™ì œì–´ (ë””ë²„ê·¸)")
    print("="*60 + "\n")
    
    controller = VisionController()
    
    try:
        controller.connect_robot()
        input("\nì¤€ë¹„ë˜ë©´ Enter...")
        controller.run(duration=30.0)
        print("="*60)
        print("âœ“ ì™„ë£Œ!")
        print("="*60)
    
    except Exception as e:
        print(f"\nâœ— ì—ëŸ¬: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        controller.disconnect_robot()


if __name__ == "__main__":
    main()
